{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e52c71ee",
   "metadata": {},
   "source": [
    "Let's do **task specific** fine-tuning by adapting the Llama 3.2 1B model for sentiment classification.\n",
    "* Dataset: IMDB\n",
    "* Model : Llama 3.2 1B\n",
    "* PEFT: LoRA (Low Rank Adaptation)\n",
    "* Quantization: 8bit (cast to float-16 during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e4d59",
   "metadata": {},
   "source": [
    "We only need to modify a few lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7a0d7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from pprint import pprint\n",
    "import math\n",
    "import wandb\n",
    "\n",
    "#hf\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import LlamaConfig, LlamaForCausalLM,LlamaForSequenceClassification\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe1fc0b",
   "metadata": {},
   "source": [
    "Upgrade the transformers, PEFT, and accelerate packages to the specified versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3ed4552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.45.2\n",
      "0.13.2\n",
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "import transformers, peft, accelerate\n",
    "print(transformers.__version__)\n",
    "print(peft.__version__)\n",
    "print(accelerate.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17477428",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9ff77c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset('stanfordnlp/imdb')\n",
    "_ = ds.pop('unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4d5a440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f9f415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassLabel(names=['neg', 'pos'], id=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[\"train\"].features['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be5c92",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72c7ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id,model_max_length=1024)\n",
    "# set pad token id\n",
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22ea502a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    example = tokenizer(example['text'],padding=False,truncation=True)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c029732a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 25000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize,batched=True,num_proc=12, remove_columns=['text'])\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c584cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 22500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['label', 'input_ids', 'attention_mask'],\n",
      "        num_rows: 2500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds_split = tokenized_ds['train'].train_test_split(test_size=0.1,seed=42)\n",
    "print(ds_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bef3ef",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "961f9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader\n",
    "data_collator = DataCollatorWithPadding(tokenizer,padding=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6322cf",
   "metadata": {},
   "source": [
    "## Inference on Model with Random Intialization of weights in the classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef6e9da",
   "metadata": {},
   "source": [
    "We expect a poor performance by the model (irrespective of the representation from the underlying model migh be good enough!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e59a572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-1B and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_id,num_labels=2,\n",
    "                                                           pad_token_id=tokenizer.eos_token_id,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2678601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForSequenceClassification(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (score): Linear(in_features=2048, out_features=2, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dbbfd951",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.id2label = {0:\"NEGATIVE\",1:\"POSITIVE\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0852bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "classifier = TextClassificationPipeline(model=model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       framework='pt',\n",
    "                                       task=\"sentiment-analysis\",\n",
    "                                       device = \"cuda\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fee58776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.982572078704834}]\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie is good.\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7974b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.997260332107544}]\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie is really bad..nothing new to hook us\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d2e36806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.971861720085144}]\n"
     ]
    }
   ],
   "source": [
    "text = \"Very bad movie with no good story\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05148561",
   "metadata": {},
   "source": [
    "Let us fine tune the classification head (treating the model as feature extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9037830",
   "metadata": {},
   "source": [
    "## Fine-tune the classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4362c7",
   "metadata": {},
   "source": [
    "Let's freeze the parameters of all layers except the last layer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99414246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight False\n",
      "model.layers.0.self_attn.q_proj.weight False\n",
      "model.layers.0.self_attn.k_proj.weight False\n",
      "model.layers.0.self_attn.v_proj.weight False\n",
      "model.layers.0.self_attn.o_proj.weight False\n",
      "model.layers.0.mlp.gate_proj.weight False\n",
      "model.layers.0.mlp.up_proj.weight False\n",
      "model.layers.0.mlp.down_proj.weight False\n",
      "model.layers.0.input_layernorm.weight False\n",
      "model.layers.0.post_attention_layernorm.weight False\n",
      "model.layers.1.self_attn.q_proj.weight False\n",
      "model.layers.1.self_attn.k_proj.weight False\n",
      "model.layers.1.self_attn.v_proj.weight False\n",
      "model.layers.1.self_attn.o_proj.weight False\n",
      "model.layers.1.mlp.gate_proj.weight False\n",
      "model.layers.1.mlp.up_proj.weight False\n",
      "model.layers.1.mlp.down_proj.weight False\n",
      "model.layers.1.input_layernorm.weight False\n",
      "model.layers.1.post_attention_layernorm.weight False\n",
      "model.layers.2.self_attn.q_proj.weight False\n",
      "model.layers.2.self_attn.k_proj.weight False\n",
      "model.layers.2.self_attn.v_proj.weight False\n",
      "model.layers.2.self_attn.o_proj.weight False\n",
      "model.layers.2.mlp.gate_proj.weight False\n",
      "model.layers.2.mlp.up_proj.weight False\n",
      "model.layers.2.mlp.down_proj.weight False\n",
      "model.layers.2.input_layernorm.weight False\n",
      "model.layers.2.post_attention_layernorm.weight False\n",
      "model.layers.3.self_attn.q_proj.weight False\n",
      "model.layers.3.self_attn.k_proj.weight False\n",
      "model.layers.3.self_attn.v_proj.weight False\n",
      "model.layers.3.self_attn.o_proj.weight False\n",
      "model.layers.3.mlp.gate_proj.weight False\n",
      "model.layers.3.mlp.up_proj.weight False\n",
      "model.layers.3.mlp.down_proj.weight False\n",
      "model.layers.3.input_layernorm.weight False\n",
      "model.layers.3.post_attention_layernorm.weight False\n",
      "model.layers.4.self_attn.q_proj.weight False\n",
      "model.layers.4.self_attn.k_proj.weight False\n",
      "model.layers.4.self_attn.v_proj.weight False\n",
      "model.layers.4.self_attn.o_proj.weight False\n",
      "model.layers.4.mlp.gate_proj.weight False\n",
      "model.layers.4.mlp.up_proj.weight False\n",
      "model.layers.4.mlp.down_proj.weight False\n",
      "model.layers.4.input_layernorm.weight False\n",
      "model.layers.4.post_attention_layernorm.weight False\n",
      "model.layers.5.self_attn.q_proj.weight False\n",
      "model.layers.5.self_attn.k_proj.weight False\n",
      "model.layers.5.self_attn.v_proj.weight False\n",
      "model.layers.5.self_attn.o_proj.weight False\n",
      "model.layers.5.mlp.gate_proj.weight False\n",
      "model.layers.5.mlp.up_proj.weight False\n",
      "model.layers.5.mlp.down_proj.weight False\n",
      "model.layers.5.input_layernorm.weight False\n",
      "model.layers.5.post_attention_layernorm.weight False\n",
      "model.layers.6.self_attn.q_proj.weight False\n",
      "model.layers.6.self_attn.k_proj.weight False\n",
      "model.layers.6.self_attn.v_proj.weight False\n",
      "model.layers.6.self_attn.o_proj.weight False\n",
      "model.layers.6.mlp.gate_proj.weight False\n",
      "model.layers.6.mlp.up_proj.weight False\n",
      "model.layers.6.mlp.down_proj.weight False\n",
      "model.layers.6.input_layernorm.weight False\n",
      "model.layers.6.post_attention_layernorm.weight False\n",
      "model.layers.7.self_attn.q_proj.weight False\n",
      "model.layers.7.self_attn.k_proj.weight False\n",
      "model.layers.7.self_attn.v_proj.weight False\n",
      "model.layers.7.self_attn.o_proj.weight False\n",
      "model.layers.7.mlp.gate_proj.weight False\n",
      "model.layers.7.mlp.up_proj.weight False\n",
      "model.layers.7.mlp.down_proj.weight False\n",
      "model.layers.7.input_layernorm.weight False\n",
      "model.layers.7.post_attention_layernorm.weight False\n",
      "model.layers.8.self_attn.q_proj.weight False\n",
      "model.layers.8.self_attn.k_proj.weight False\n",
      "model.layers.8.self_attn.v_proj.weight False\n",
      "model.layers.8.self_attn.o_proj.weight False\n",
      "model.layers.8.mlp.gate_proj.weight False\n",
      "model.layers.8.mlp.up_proj.weight False\n",
      "model.layers.8.mlp.down_proj.weight False\n",
      "model.layers.8.input_layernorm.weight False\n",
      "model.layers.8.post_attention_layernorm.weight False\n",
      "model.layers.9.self_attn.q_proj.weight False\n",
      "model.layers.9.self_attn.k_proj.weight False\n",
      "model.layers.9.self_attn.v_proj.weight False\n",
      "model.layers.9.self_attn.o_proj.weight False\n",
      "model.layers.9.mlp.gate_proj.weight False\n",
      "model.layers.9.mlp.up_proj.weight False\n",
      "model.layers.9.mlp.down_proj.weight False\n",
      "model.layers.9.input_layernorm.weight False\n",
      "model.layers.9.post_attention_layernorm.weight False\n",
      "model.layers.10.self_attn.q_proj.weight False\n",
      "model.layers.10.self_attn.k_proj.weight False\n",
      "model.layers.10.self_attn.v_proj.weight False\n",
      "model.layers.10.self_attn.o_proj.weight False\n",
      "model.layers.10.mlp.gate_proj.weight False\n",
      "model.layers.10.mlp.up_proj.weight False\n",
      "model.layers.10.mlp.down_proj.weight False\n",
      "model.layers.10.input_layernorm.weight False\n",
      "model.layers.10.post_attention_layernorm.weight False\n",
      "model.layers.11.self_attn.q_proj.weight False\n",
      "model.layers.11.self_attn.k_proj.weight False\n",
      "model.layers.11.self_attn.v_proj.weight False\n",
      "model.layers.11.self_attn.o_proj.weight False\n",
      "model.layers.11.mlp.gate_proj.weight False\n",
      "model.layers.11.mlp.up_proj.weight False\n",
      "model.layers.11.mlp.down_proj.weight False\n",
      "model.layers.11.input_layernorm.weight False\n",
      "model.layers.11.post_attention_layernorm.weight False\n",
      "model.layers.12.self_attn.q_proj.weight False\n",
      "model.layers.12.self_attn.k_proj.weight False\n",
      "model.layers.12.self_attn.v_proj.weight False\n",
      "model.layers.12.self_attn.o_proj.weight False\n",
      "model.layers.12.mlp.gate_proj.weight False\n",
      "model.layers.12.mlp.up_proj.weight False\n",
      "model.layers.12.mlp.down_proj.weight False\n",
      "model.layers.12.input_layernorm.weight False\n",
      "model.layers.12.post_attention_layernorm.weight False\n",
      "model.layers.13.self_attn.q_proj.weight False\n",
      "model.layers.13.self_attn.k_proj.weight False\n",
      "model.layers.13.self_attn.v_proj.weight False\n",
      "model.layers.13.self_attn.o_proj.weight False\n",
      "model.layers.13.mlp.gate_proj.weight False\n",
      "model.layers.13.mlp.up_proj.weight False\n",
      "model.layers.13.mlp.down_proj.weight False\n",
      "model.layers.13.input_layernorm.weight False\n",
      "model.layers.13.post_attention_layernorm.weight False\n",
      "model.layers.14.self_attn.q_proj.weight False\n",
      "model.layers.14.self_attn.k_proj.weight False\n",
      "model.layers.14.self_attn.v_proj.weight False\n",
      "model.layers.14.self_attn.o_proj.weight False\n",
      "model.layers.14.mlp.gate_proj.weight False\n",
      "model.layers.14.mlp.up_proj.weight False\n",
      "model.layers.14.mlp.down_proj.weight False\n",
      "model.layers.14.input_layernorm.weight False\n",
      "model.layers.14.post_attention_layernorm.weight False\n",
      "model.layers.15.self_attn.q_proj.weight False\n",
      "model.layers.15.self_attn.k_proj.weight False\n",
      "model.layers.15.self_attn.v_proj.weight False\n",
      "model.layers.15.self_attn.o_proj.weight False\n",
      "model.layers.15.mlp.gate_proj.weight False\n",
      "model.layers.15.mlp.up_proj.weight False\n",
      "model.layers.15.mlp.down_proj.weight False\n",
      "model.layers.15.input_layernorm.weight False\n",
      "model.layers.15.post_attention_layernorm.weight False\n",
      "model.norm.weight False\n",
      "score.weight True\n"
     ]
    }
   ],
   "source": [
    " for name,param in model.named_parameters():    \n",
    "     if name != \"score.weight\":\n",
    "        param.requires_grad = False\n",
    "     print(name,param.requires_grad)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f984c",
   "metadata": {},
   "source": [
    "Count the number of trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "65674c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:4096\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "for param in model.parameters():   \n",
    "    if param.requires_grad:\n",
    "        num_parameters += param.numel()\n",
    "print(f'Number of Parameters:{num_parameters}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d320e07",
   "metadata": {},
   "source": [
    "Let us load evaluation metrics (accuracy in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f23dcab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9550c4",
   "metadata": {},
   "source": [
    "Batch of size 8 should work in Colab as we fine tune only the last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f167b75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments( output_dir='llma32_imdb_ft',\n",
    "                                  eval_strategy=\"steps\",\n",
    "                                  eval_steps=100,\n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=12,\n",
    "                                  per_device_eval_batch_size=12,\n",
    "                                  bf16=False,\n",
    "                                  fp16=True,\n",
    "                                  tf32=False,\n",
    "                                  gradient_accumulation_steps=1,\n",
    "                                  adam_beta1=0.9,\n",
    "                                  adam_beta2=0.999,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_dir='logs',\n",
    "                                  logging_strategy=\"steps\",\n",
    "                                  logging_steps = 100,\n",
    "                                  save_steps=100,\n",
    "                                  save_total_limit=20,\n",
    "                                  report_to='none',\n",
    "                                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec216212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=model,\n",
    "                  args = training_args,\n",
    "                 train_dataset=ds_split[\"train\"],\n",
    "                 eval_dataset=ds_split[\"test\"],\n",
    "                 compute_metrics=compute_metrics,\n",
    "                 data_collator = data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c2c1c",
   "metadata": {},
   "source": [
    "```python\n",
    "results = trainer.train() # make this a code cell to execute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4369d8c2",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/ft_last_layer_llama321b.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700f688",
   "metadata": {},
   "source": [
    "* A simple linear network is able to acheive the training accuracy of 85.8%. \n",
    "* It Implies that the model is able to produce a good representation of the input sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9db097ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "classifier = TextClassificationPipeline(model=model,\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       framework='pt',\n",
    "                                       task=\"sentiment-analysis\",\n",
    "                                       device = \"cuda\"\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6a768",
   "metadata": {},
   "source": [
    "## Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7cf03338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.8752306699752808}]\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie is good.\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "57d590dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.6252825260162354}]\n"
     ]
    }
   ],
   "source": [
    "text = \"The movie is really bad..nothing new to hook us\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91e8147e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9068325161933899}]\n"
     ]
    }
   ],
   "source": [
    "text = \"Very bad movie with no good story\"\n",
    "prediction = classifier(text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adfd50d2",
   "metadata": {},
   "source": [
    "For the text `\"Very bad movie with no good story\"`, the fine tuned model predicts it as negative sentiment with high confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17ed0d1",
   "metadata": {},
   "source": [
    "## What is next?\n",
    "* Train the model with the LoRA adapters and see how it performs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(hf)",
   "language": "python",
   "name": "tocr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
