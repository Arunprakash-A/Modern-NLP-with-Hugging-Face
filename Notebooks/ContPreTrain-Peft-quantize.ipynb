{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b905fc9-2ee5-413c-bda3-6112b7af1763",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Continual Pretraining of Llama 3.2 1B</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d79f2ae-530e-4de1-9faa-3712b7d47018",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We will continually pre-train the Llama 3.2 1B model on the Tamil subset of the Sangraha dataset from AI4Bharat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877f5972-5d6f-40ed-b6a9-95ef09b2213e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We are going to use an L4 GPU with 48 GB of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7680a8-fab2-45cf-82cf-3ba34215400d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Training all the model's parameters requires significant amount of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0991b95f-5512-473c-b6c7-602aec3d490b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Therefore, we will train the model using LoRa, one of several parameter-efficient fine-tuning techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58a711bc-3ea2-45b0-9a8e-2a89455d644b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import math\n",
    "import wandb\n",
    "\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e334d7b-1e8a-4597-8b71-dac9df7b1e7a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "wandb.init(\n",
    "    project=\"DLP-W4-CPT-Node-1\",\n",
    "    config={\n",
    "        \"batch_size\":4,\n",
    "        \"dataset\": \"Sangraha\",\n",
    "    },\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40182e1d-581f-4748-a919-bb2e5a77aa53",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Load the dataset </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32adbb29-e719-4e4a-b7f5-23bf8248945d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's load a portion of sangraha dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cd2bb3-c206-42c2-a413-12562f04889d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42bceb8675c14614bf5fbd2861c0eeb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/11.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24b98c906bcf4ae8920299cc01cabe21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/358M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62c2c2aa46d44dd3ae943e870dbe01d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['doc_id', 'text', 'type'],\n",
      "    num_rows: 149796\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('ai4bharat/sangraha',data_files=\"https://huggingface.co/datasets/ai4bharat/sangraha/resolve/main/verified/tam/data-0.parquet\")['train']\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f727632c-2456-46d9-9b64-ca2141a725a8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Load Llama 3.2 1b tokenizer </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89ea02b8-4b2c-44a8-823d-955e85ff15c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 128000\n",
      "Context length: 131072\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-1B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "print(f'Vocab size: {tokenizer.vocab_size}')\n",
    "print(f'Context length: {tokenizer.model_max_length}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a347175f-11f1-4c19-a53b-91457b011a50",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.model_max_length = 1024\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b1bf07-af61-4b43-bb34-6f66be78f51a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's compute the approximate fertility score of the tokenizer for the Tamil language "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e2605f5-95fb-41e1-ae8a-01958dcbaed6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 47\n"
     ]
    }
   ],
   "source": [
    "example = ds[1]\n",
    "num_words = len(example['text'].split())\n",
    "print(f'Number of words: {num_words}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c145eae-4486-4bf3-8257-466b92a89b15",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 521\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(example['text'])['input_ids']\n",
    "print(f'Number of tokens: {len(input_ids)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18616621-09b3-4259-ab87-8a43254b0e32",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fertility rate is: 11.085106382978724\n"
     ]
    }
   ],
   "source": [
    "print(f'The fertility rate is: {len(input_ids)/num_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08169993-52df-46cd-9cf3-e70f0089561b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Typically, the fertility score for the tokenizer is quite high for Indic languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a8cb3c-bb94-4252-adae-10212b7839d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The fertility score is high (meaning that, every word is split into 11 tokens on average). Of course, to get the correct score, we have to use all the samples from the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0e465d-591d-4fa4-aa48-0318372feb37",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "செய்முறைஃ\n",
      "பச்சரிசி மற்றும் பச்சைப்பயறை ஒன்றாக சேர்த்து ஒரு மணி நேரம் ஊற வைக்கவும். ஊறிய அரிசி, பயறுடன், தேங்காய் துருவல், காய்ந்த மிளகாய், பெருங்காயத்தூள், இஞ்சி, கொத்தமல்லி, கறிவேப்பிலை, உப்பு சேர்த்து தோசை மாவு பதத்தில் அரைத்து கொள்ளவும்.\n",
      " அடுப்பில் தோசைக்கல்லை வைத்து எண்ணெய் ஊற்றி காய்ந்ததும் அரைத்து வைத்திருக்கும் மாவை ஊற்றி சுட்டு எடுக்கவும். சுவையான பச்சை பயறு தோசை ரெசிபி ரெடி. \n"
     ]
    }
   ],
   "source": [
    "print(example['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58afffe-d819-4a93-9a6a-6f48e6ee26b6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|begin_of_text|>', 'à®', 'ļ', 'à¯', 'Ĩ', 'à®', '¯', 'à¯įà®', '®', 'à¯ģ', 'à®', '±', 'à¯', 'Ī', 'à®', 'ĥ', 'Ċ', 'à®', 'ª', 'à®', 'ļ', 'à¯įà®', 'ļ', 'à®', '°', 'à®¿à®', 'ļ', 'à®¿', 'Ġà®', '®', 'à®', '±', 'à¯įà®', '±', 'à¯ģ', 'à®', '®', 'à¯į', 'Ġà®', 'ª', 'à®', 'ļ', 'à¯įà®', 'ļ', 'à¯', 'Ī', 'à®']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|>செய்முறைஃ\\nபச்சரிசி மற்றும் பச்சைப்பயறை ஒன்றாக சேர்த்து ஒரு மணி நேரம் ஊற வைக்கவும். ஊறிய அரிசி, பயறுடன், தேங்காய் துருவல், காய்ந்த மிளகாய், பெருங்காயத்தூள், இஞ்சி, கொத்தமல்லி, கறிவேப்பிலை, உப்பு சேர்த்து தோசை மாவு பதத்தில் அரைத்து கொள்ளவும்.\\n அடுப்பில் தோசைக்கல்லை வைத்து எண்ணெய் ஊற்றி காய்ந்ததும் அரைத்து வைத்திருக்கும் மாவை ஊற்றி சுட்டு எடுக்கவும். சுவையான பச்சை பயறு தோசை ரெசிபி ரெடி. '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(tokens[0 7])\n",
    "tokenizer.decode(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfe0e7-6199-4474-bd4d-facc9341bdd1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Anyway, let us go with this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf82e922-99c9-40d2-b446-7f567433dbca",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(example):\n",
    "    example = tokenizer(example['text'],padding=False,truncation=True)\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa57b89a-9664-48d1-a731-855ed7efaf1a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd7bdc8c9a624b0ba8075e09e11c4187",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=12):   0%|          | 0/149796 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 149796\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = ds.map(tokenize,batched=True,num_proc=12, remove_columns=['doc_id', 'text', 'type'])\n",
    "print(tokenized_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1d57b8-dd30-4cf5-8d21-63cdd45b46a5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Packing Sequence </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a374f2d-6d25-4a34-8210-e287ce0f5d61",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def concatenate_and_chunk(examples):    \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20766ddf-0356-4935-9c4d-27c678839897",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_chunked = ds.map(concatenate_and_chunk,\n",
    "                    batch_size=1000,\n",
    "                    batched=True,\n",
    "                    num_proc=12,\n",
    "                    remove_columns=['doc_id', 'text', 'type']\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051945f6-0501-49c9-add0-02671b2557fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_chunked.save_to_disk('tamil_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c9d06ab-090b-413c-ba4b-8c0f9e6c759a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds_chunked = load_from_disk('tamil_ds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ddded4-3800-45a6-974f-71862848dffa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask'],\n",
      "    num_rows: 483683\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds_chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f8aeb03-b267-4ca7-a218-17c54766e9b3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 483199\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 484\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "ds_split = ds_chunked.train_test_split(test_size=0.001,seed=42)\n",
    "print(ds_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5562713-bd6e-48a1-9ca0-1257bdd2fd88",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Data Collator </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44fd6450-9455-4e92-b84b-6f43b2076ea6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataloader\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer,mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572a9557-eca3-412e-b47b-716fbf3d65b7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> Loading LLama 3.2 1b Model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce62bb0-be63-4b0c-b4c3-5fa490b74d8d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "LLama 3.2 is a gated model, and you need permission to access the model weights. <br>\n",
    "(**Note:** You will receive access an hour or two after submitting the form from your Hugging Face account.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27722790-4bc8-4a34-80ee-7ef5a5f90c6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "You can find details about the model, such as its architecture, performance, and more,  [here](https://huggingface.co/meta-llama/Llama-3.2-1B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dafee8-6c29-4d3d-b0b2-16cc014ae650",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Here are some important details: <br>\n",
    "* Number of parameters:  **1 Billion** (actually, 1.23 Billion)\n",
    "* Context length: 128 K (actually, 131K)\n",
    "* Vocabulary size: 128 K\n",
    "* Input modalities: Multilingual\n",
    "* Token count: **9T**\n",
    "* Knowledge cutoff: Dec 2023\n",
    "* GPU clusters: **916K** GPU Hours on H100 80GB "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a936bcd-2fba-4a6e-aa44-e1b21c287222",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Since we are continuing the pre-training process, we load the model with a CausalLM head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03a34011-2d18-4550-bc55-8bf0da1c4ad5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id,pad_token_id=tokenizer.eos_token_id)\n",
    "# pad_token needed in general, otherwise raises an error (makes sense!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9e3367-22f6-41ea-b596-58feee26697b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's look at the configuration and architecture details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be0cef34-25cf-4c03-94ef-5b41a90f3ba9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaConfig {\n",
      "  \"_name_or_path\": \"meta-llama/Llama-3.2-1B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"head_dim\": 64,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 16,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128001,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "configuration = model.config\n",
    "print(configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0eb9934b-0e03-43fe-9e74-5a612df2c223",
   "metadata": {
    "editable": true,
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefdb4e0-7870-4f22-adcb-78b3edc99d6e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "It uses decoder layers (like GPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26278e58-c49a-4cfb-ba2b-3714de5d35ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "However, the model uses Relative Position Embeddings (RPE) that are added in the attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2459592-2735-4d17-ac07-1cbf35dd6d65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters:1.24 B\n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "for param in model.parameters():    \n",
    "    num_parameters += param.numel()\n",
    "print(f'Number of Parameters:{num_parameters/10**9:.2f} B')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcf614e-c7f0-488b-abed-637323fa9bf3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's calculate the memory requirement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9693e48-2329-4d93-a3ce-c54fcf54f1ec",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(model.dtype) # 4 bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783e0735-6596-4f71-8fff-cf33f5f9065e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Therefore, the memory requirement is simply the number of parameters multiplied by the data type used to store each parameter. <br> (**Note:** We also need to store additional parameters, such as statistics for normalization, that do not require gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c99ec89-d476-4a2e-b5ce-1f729ffffa6c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9432576\n"
     ]
    }
   ],
   "source": [
    "mem_in_gb = num_parameters*4/1e9\n",
    "print(mem_in_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d44ca00-1d28-40e2-9865-162f3f51e0cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can also get the info directly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92cc882b-1acd-440f-9fd4-7866a7819f68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.943259776\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c7ac0-6fd2-4779-9709-6e7a46bf3c4d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* The model needs about 5GB of memory.\n",
    "* However, during training, additional memory is needed for storing gradients, which depends on the type of optimizer used.\n",
    "* Additionally, GPU kernels consume some memory, typically between 2 to 4 GB, depending on the type of GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d28103ee-296d-4a5d-b785-8db646e967f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Memory requirement per sample: 20.7730304 GB\n"
     ]
    }
   ],
   "source": [
    "param_model = num_parameters*4/1e9\n",
    "adam_opt = 3*param_model # for storing moments\n",
    "kernel = 1\n",
    "bs = 1 # batch size\n",
    "print(f'Total Memory requirement per sample: {(param_model+adam_opt+kernel)*bs} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04767aff-4217-41f2-b4a1-f4c96c7b86e1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "We need at least 21 GB of memory to train the model with a batch size of 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf69497-71f6-4319-949f-8f2d06067d1e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Therefore, for this demonstration, we will use a single-node L4 GPU with 2 GPU instances, each having 24 GB of GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b241d693-e656-49d3-91b1-1f0ce11420dc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "How do we increase the batch size per GPU device from 1 to at least 2?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f0e819-c73b-4717-9535-dd3ca0560b10",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Of course, we can use a gradient accumulation strategy; however, this will increase the training time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8267b819-64bd-4917-aaa7-35961a0bb34d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The answer is : **PEFT** adapters (optionally combined with quantization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9507b0-00ce-42f2-9a23-720c18730860",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\"> PEFT: LoRA</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5f4f4e-dcc5-410a-bc7b-4fa0c035f3d0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Before we continue with the pre-training, let's see how the model generates a coherent text based on the given prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "218253c5-00b3-481f-9564-ea7fe65f9fd8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['I was reading Feynman\\'s lecture on physics. He talks about 2 different ways to look at the world, the \"old\" way and the \"new\" way. In the \"old\" way, we look at the world and see that the universe is governed by laws. In the \"new\" way,']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"I was reading Feynman's lecture on physics. He talks about \"\n",
    "inputs = tokenizer(prompt,return_tensors='pt',padding=True) \n",
    "outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=10, top_p=0.95)\n",
    "tokeni zer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "294aa79b-ca65-4434-a975-de43716d4a9e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானிலை \\xa0கண்ணாடியில் பெய்யும் வரை இருக்கிறது. இதையால் புறநகர்ப் பகுதிகள் அதிக விளை']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானிலை \"\n",
    "inputs = tokenizer(prompt,return_tensors='pt',padding=True) \n",
    "# set max tokens to a little higher as the words are split into 11 tokens on average\n",
    "outputs = model.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa1b36-c201-46d2-b3e3-3031aec55026",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Great! Let's continue the pre-training of the model on the Tamil subset of the Sangraha dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34485fb2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/lora_1.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c48b8f-9d61-4677-adfe-7117046c1a6d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "**Forward Pass:** <br>\n",
    "$$h=Wx+\\Delta Wx = Wx+BAx \\quad \\text{where} \\quad W \\in \\mathbb{R}^{d \\times k}, B \\in \\mathbb{R}^{d \\times r}, A \\in \\mathbb{R}^{r \\times k}$$\n",
    "* $W$ is a pre-trained weight matrix  (during training, $W$ is frozen and **does not** receive gradient updates)\n",
    "* $A$ is initialized randomly (say, Gaussian)\n",
    "* $B$ is initialized to zero\n",
    "* $r$ is the rank\n",
    "* $\\Delta W$ is scaled by $\\frac{\\alpha}{r}$ after the first iteration, where $\\alpha$ is a constant\n",
    "\n",
    "**Benefit of LoRa**: <br>\n",
    "* Switching between tasks only by swapping the LoRA weights instead of all the parameters.\n",
    "* This allows for the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM\n",
    "\n",
    "**Paper**: https://arxiv.org/pdf/2106.09685 <br>\n",
    "**HF Doc**: https://huggingface.co/docs/peft/v0.7.1/en/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65896787-8850-4d92-ad43-cfa2e72a1738",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, LoraModel\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7448cbfe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "In general, we can add adapters to any torch modules (nn.Linear, Conv1D,..). For example,\n",
    "```\n",
    "target_modules = ['q_proj','k_proj','v_proj','o_proj','gate_proj','down_proj','up_proj','lm_head']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "17ffba50-4ce3-4bb5-91a8-319916f36d95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c849e52e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "The trainable parameters comprise only 0.13% of the total parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "03f195cb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=2048, out_features=512, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "              (up_proj): Linear(in_features=2048, out_features=8192, bias=False)\n",
      "              (down_proj): Linear(in_features=8192, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229cfb43",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "* LoRA is only applied to the two porjection layers.\n",
    "* For \"q_proj\": $A \\in \\mathbb{R}^{2048 \\times 16}$ and $B \\in \\mathbb{R}^{16 \\times 2048}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ba7ebe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Once again, let us quickly verify the number of learnable parameters for our own satisfaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34a7ddfc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters of original model:1237518336 \n"
     ]
    }
   ],
   "source": [
    "num_parameters = 0\n",
    "for param in model.parameters():    \n",
    "    num_parameters += param.numel()\n",
    "print(f'Number of Parameters of original model:{num_parameters} ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "97b943a4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Parameters of LoRA model:1703936 \n"
     ]
    }
   ],
   "source": [
    "num_parameters_lora = 0\n",
    "for param in lora_model.parameters(): \n",
    "    if param.requires_grad:\n",
    "        num_parameters_lora += param.numel()\n",
    "print(f'Number of Parameters of LoRA model:{num_parameters_lora} ') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae69c8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Note, however, that we still need to keep the entire model in GPU memory. This requires about 5 GB of RAM. Additionally, we need to store the activation values of all layers, which consumes a significant amount of memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec48926a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Before proceeding further, let's check a few things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e9830f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'default': LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.2-1B', revision=None, task_type=<TaskType.CAUSAL_LM: 'CAUSAL_LM'>, inference_mode=False, r=16, target_modules={'v_proj', 'q_proj'}, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.peft_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b45a25",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\">  Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1353bd23",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments( output_dir='lora_llama_1b_ct',\n",
    "                                  eval_strategy=\"steps\",\n",
    "                                  eval_steps=100,\n",
    "                                  num_train_epochs=1,\n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  bf16=False,\n",
    "                                  fp16=True,\n",
    "                                  tf32=False,\n",
    "                                  gradient_accumulation_steps=1,\n",
    "                                  adam_beta1=0.9,\n",
    "                                  adam_beta2=0.999,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  weight_decay=0.01,\n",
    "                                  logging_dir='logs',\n",
    "                                  logging_strategy=\"steps\",\n",
    "                                  logging_steps = 100,\n",
    "                                  save_steps=100,\n",
    "                                  save_total_limit=20,\n",
    "                                  report_to='none',\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d7042618",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model=lora_model,\n",
    "                  args = training_args,\n",
    "                 train_dataset=ds_split[\"train\"],\n",
    "                 eval_dataset=ds_split[\"test\"],\n",
    "                 data_collator = data_collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4c4c47",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "```python\n",
    "results = trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90801e5-d36a-42bc-9d1e-67ef35403fad",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/Lora_train_loss.png\" width=\"600\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c73011-714b-4ae2-97f4-b29ead7b3c3b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Arunprakash-A/Modern-NLP-with-Hugging-Face/refs/heads/main/Notebooks/images/Lora_validation_loss.png\" width=\"600\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88eacd8-126c-4d63-b6ba-f7cfb48ac55b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "The model used approximately 10 GB of GPU memory with a batch size of 1, which is less than half of what the original model requires (over 22 GB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf8582-7027-4d11-b25c-489cf4b9f9b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "**WARNING**\n",
    "* We loaded the original model weights and stored them in a variable `model`\n",
    "* We then applied LoRA and stored the resulting model in the variable `lora_model`\n",
    "* By design, **The `model` is modified `in-place`** (to save memory?)\n",
    "* This is not an issue when using a script; however, it can create problems in notebooks if we execute the `model` after executing `lora_model`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7238d6-e1ec-46f0-a0ba-b038dc2c6faf",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Let's load the model checkpoint (after processing 92,400 samples or 94 milllion tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "384bafc4-a5cf-47be-b7a7-2f4e6eb3adbe",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cpt = AutoModelForCausalLM.from_pretrained('checkpoint-15400/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "25b560c3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானிலை நிறுவனம் கூறுகிறது. பெரும்பாலான இடங்களில் கண மழை பெய்யும் காரணம் இது.']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"இன்று இடியுடன் கூடிய கண மழை பெய்யும் என சென்னை வானில\"\n",
    "inputs = tokenizer(prompt,return_tensors='pt',padding=True) \n",
    "# set max tokens to a little higher as the words are split into 11 tokens on average\n",
    "outputs = model_cpt.generate(**inputs, max_new_tokens=100, do_sample=True, top_k=10, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42035ae-c575-4aff-a5bd-240922fb1113",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* As usual, we can store the model using the `.save_pretrained` method and load the peft model back with `from peft import PeftModel`\n",
    "* By default, the PeftModel is set for **inference**, but if you’d like to train the adapter further you can set `is_trainable=True.`\n",
    "```python\n",
    "lora_model = PeftModel.from_pretrained(model, \"path/to/model\", is_trainable=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a320d9-deb9-48a0-9e95-d2a8681e10ed",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "What happens to the model's ability to complete a given prompt coherently after continual pre-training on a potentially domain-specific dataset? Does it retain its world knowledge? Although we haven't trained the model on a larger dataset, we hope that it still retains its general knowledge. Let’s see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6c673",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"I was reading Feynman's lecture on physics. He talks about \"\n",
    "inputs = tokenizer(prompt,return_tensors='pt',padding=True) \n",
    "outputs = model_cpt.generate(**inputs, max_new_tokens=50, do_sample=True, top_k=10, top_p=0.95)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a2786-1d53-4760-934b-c03826ed90c6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Following is the response we got: <br> \n",
    "[\"I was reading Feynman's lecture on physics. He talks about 2 different approaches to solving problems. One is to use the mathematics and the other is to use the physics. I have to say I think the physics approach is more useful, but I think the mathematics approach is also useful.\\nI think it is useful]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acba171-62b2-4fec-ba22-e979248a92f1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<h1 style=\"color:Tomato;\">Quantization</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a8eb10-c91d-486d-a462-1ddb6530681f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "We can further reduce memory requirement (10 GB with LoRA) by quantizing the model parameters and adding adapters to the quantized model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd15af9a-89fe-4c55-ab74-316282f290fc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce41d10-c487-441c-b85d-22ca2375cb86",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Load the model parameters in 8bit precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf8dafe-636e-47dc-9579-e35a1d220a1c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config, \n",
    "                                                  pad_token_id=tokenizer.eos_token_id,\n",
    "                                                  device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b245de03-5a26-4d92-9f4b-732d9fd680c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
      "    (layers): ModuleList(\n",
      "      (0-15): 16 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaAttention(\n",
      "          (q_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "          (v_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "          (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "          (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "          (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "    (rotary_emb): LlamaRotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_8bit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8baf95",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's add adapters to fine-tune the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f973a448",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType,LoraModel\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23c84dc0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model( , lora_config)\n",
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f285ff78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): LlamaForCausalLM(\n",
      "      (model): LlamaModel(\n",
      "        (embed_tokens): Embedding(128256, 2048, padding_idx=128001)\n",
      "        (layers): ModuleList(\n",
      "          (0-15): 16 x LlamaDecoderLayer(\n",
      "            (self_attn): LlamaAttention(\n",
      "              (q_proj): lora.Linear8bitLt(\n",
      "                (base_layer): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=2048, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "              (v_proj): lora.Linear8bitLt(\n",
      "                (base_layer): Linear8bitLt(in_features=2048, out_features=512, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=2048, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): Linear8bitLt(in_features=2048, out_features=2048, bias=False)\n",
      "              (rotary_emb): LlamaRotaryEmbedding()\n",
      "            )\n",
      "            (mlp): LlamaMLP(\n",
      "              (gate_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "              (up_proj): Linear8bitLt(in_features=2048, out_features=8192, bias=False)\n",
      "              (down_proj): Linear8bitLt(in_features=8192, out_features=2048, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "            (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
      "        (rotary_emb): LlamaRotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(lora_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c685a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Now we can train the model by using `Trainer` API!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bcb3e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "* This approach requires approximately **6 GB** of GPU memory (with a batch size of 1)\n",
    "* In contrast, LoRA without quantization requires about **10 GB** of GPU memory (also with a batch size of 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef659192",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Finally, we were able to continue the pre-training of Llama 3.2 1B with a batch size of 16 on the L4 GPU node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab99359b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "Next, we will explore how to perform task-specific fine-tuning of the pre-trained model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
